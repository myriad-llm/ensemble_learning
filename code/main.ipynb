{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "\n",
    "import functions\n",
    "\n",
    "\n",
    "ALL = False\n",
    "NO_SMOTE = True\n",
    "SUB_PROCESSED_DIR = 'processed'\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "dtypes = {\n",
    "    'msisdn': 'str',\n",
    "    'start_time': 'str',\n",
    "    'end_time': 'str',\n",
    "    'call_event': 'category',\n",
    "    'other_party': 'str',\n",
    "    'ismultimedia': 'category',\n",
    "    'home_area_code': 'str',\n",
    "    'visit_area_code': 'str',\n",
    "    'called_home_code': 'str',\n",
    "    'called_code': 'str',\n",
    "    'a_serv_type': 'int',\n",
    "    'long_type1': 'int',\n",
    "    'roam_type': 'int',\n",
    "    'a_product_id': 'str',\n",
    "    'open_datetime': 'str',\n",
    "    'call_duration': 'int32',\n",
    "    'cfee': 'float64',\n",
    "    'lfee': 'float64',\n",
    "    'hour': 'int8',\n",
    "    'dayofweek': 'int',\n",
    "    'phone1_type': 'int',\n",
    "    'phone2_type': 'int',\n",
    "    'phone1_loc_city': 'str',\n",
    "    'phone1_loc_province': 'str',\n",
    "    'phone2_loc_city': 'str',\n",
    "    'phone2_loc_province': 'str',\n",
    "    'update_time': 'str',\n",
    "    'date': 'str',\n",
    "    'date_c': 'str',\n",
    "\n",
    "    \"phone1_loc_city_lat\": \"float64\",\n",
    "    \"phone1_loc_city_lon\": \"float64\",\n",
    "    \"phone2_loc_city_lat\": \"float64\",\n",
    "    \"phone2_loc_city_lon\": \"float64\",\n",
    "}\n",
    "\n",
    "# 判断 processed 文件夹是否存在\n",
    "import os\n",
    "if not os.path.exists(f'../self_data/{SUB_PROCESSED_DIR}'):\n",
    "    print(\"Creating processed data folder...\")\n",
    "    # 读取CSV文件\n",
    "    labeled_data = pd.read_csv('../self_data/all_trainSet_res.csv', dtype=dtypes)\n",
    "    labels = pd.read_csv('../self_data/all_trainSet_ans.csv', dtype=dtypes)\n",
    "    validation_data = pd.read_csv('../self_data/sorted_validationSet_res_with_head.csv', dtype=dtypes)\n",
    "\n",
    "    # 按照 msisdn 切分 train_data 和 test_data\n",
    "    train_data_msisdn, test_data_msisdn = train_test_split(labels['msisdn'], test_size=TEST_RATIO, random_state=42, stratify=labels['is_sa'])\n",
    "    train_data = labeled_data[labeled_data['msisdn'].isin(train_data_msisdn)]\n",
    "    train_labels = labels[labels['msisdn'].isin(train_data_msisdn)]\n",
    "    assert len(train_data['msisdn'].unique()) == len(train_data_msisdn)\n",
    "\n",
    "    test_data = labeled_data[labeled_data['msisdn'].isin(test_data_msisdn)]\n",
    "    test_labels = labels[labels['msisdn'].isin(test_data_msisdn)]\n",
    "    assert len(test_data['msisdn'].unique()) == len(test_data_msisdn)\n",
    "\n",
    "    train_data, train_labels, labels_aug = functions.augment_data_parallel(train_data, train_labels, test_labels)\n",
    "\n",
    "    # save\n",
    "    print(\"Saving processed data...\")\n",
    "    os.makedirs(f'../self_data/{SUB_PROCESSED_DIR}', exist_ok=True)\n",
    "    train_data.to_pickle(f'../self_data/{SUB_PROCESSED_DIR}/train_data.pkl')\n",
    "    train_labels.to_pickle(f'../self_data/{SUB_PROCESSED_DIR}/train_labels.pkl')\n",
    "    test_data.to_pickle(f'../self_data/{SUB_PROCESSED_DIR}/test_data.pkl')\n",
    "    test_labels.to_pickle(f'../self_data/{SUB_PROCESSED_DIR}/test_labels.pkl')\n",
    "    labels_aug.to_pickle(f'../self_data/{SUB_PROCESSED_DIR}/labels_aug.pkl')\n",
    "    validation_data.to_pickle(f'../self_data/{SUB_PROCESSED_DIR}/validation_data.pkl')\n",
    "\n",
    "else:\n",
    "    print(\"Reading processed data...\")\n",
    "    train_data = pd.read_pickle(f'../self_data/{SUB_PROCESSED_DIR}/train_data.pkl')\n",
    "    train_labels = pd.read_pickle(f'../self_data/{SUB_PROCESSED_DIR}/train_labels.pkl')\n",
    "    test_data = pd.read_pickle(f'../self_data/{SUB_PROCESSED_DIR}/test_data.pkl')\n",
    "    test_labels = pd.read_pickle(f'../self_data/{SUB_PROCESSED_DIR}/test_labels.pkl')\n",
    "    labels_aug = pd.read_pickle(f'../self_data/{SUB_PROCESSED_DIR}/labels_aug.pkl')\n",
    "    validation_data = pd.read_pickle(f'../self_data/{SUB_PROCESSED_DIR}/validation_data.pkl')\n",
    "\n",
    "labeled_data_aug = pd.concat([train_data, test_data], ignore_index=True).reindex()\n",
    "assert len(labeled_data_aug['msisdn'].unique()) == len(labels_aug['msisdn'].unique())\n",
    "\n",
    "# 转换时间格式\n",
    "labeled_data_aug['start_time'] = pd.to_datetime(labeled_data_aug['start_time'], format='%Y%m%d%H%M%S')\n",
    "labeled_data_aug['end_time'] = pd.to_datetime(labeled_data_aug['end_time'], format='%Y%m%d%H%M%S')\n",
    "labeled_data_aug['open_datetime'] = pd.to_datetime(labeled_data_aug['open_datetime'], format='%Y%m%d%H%M%S')\n",
    "labeled_data_aug['update_time'] = pd.to_datetime(labeled_data_aug['update_time'])\n",
    "labeled_data_aug['date'] = pd.to_datetime(labeled_data_aug['date'])\n",
    "\n",
    "validation_data['start_time'] = pd.to_datetime(validation_data['start_time'], format='%Y%m%d%H%M%S')\n",
    "validation_data['end_time'] = pd.to_datetime(validation_data['end_time'], format='%Y%m%d%H%M%S')\n",
    "validation_data['open_datetime'] = pd.to_datetime(validation_data['open_datetime'], format='%Y%m%d%H%M%S',errors='coerce')\n",
    "validation_data['update_time'] = pd.to_datetime(validation_data['update_time'])\n",
    "validation_data['date'] = pd.to_datetime(validation_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为每条记录添加start_time_diff，记录 start_time 与上一条记录的 start_time 之差 (单位：秒)\n",
    "start_time_diff = labeled_data_aug.groupby('msisdn')['start_time'].diff().dt.total_seconds().fillna(0).reset_index(drop=True)\n",
    "# 将该列加入到数据集中\n",
    "labeled_data_aug['start_time_diff'] = start_time_diff.copy()\n",
    "start_time_diff = validation_data.groupby('msisdn')['start_time'].diff().dt.total_seconds().fillna(0).reset_index(drop=True)\n",
    "validation_data['start_time_diff'] = start_time_diff.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "数据特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled_aug_features = functions.aggregate_features_parallel(labeled_data_aug)\n",
    "validation_features = functions.aggregate_features_parallel(validation_data)\n",
    "\n",
    "# 合并标签数据\n",
    "labeled_aug_features = labeled_aug_features.merge(labels_aug, on='msisdn', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.get_nan(labeled_aug_features), functions.get_nan(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一般只有 std 会出现 nan 值故所有的 nan 值填充为 0\n",
    "labeled_aug_features = labeled_aug_features.fillna(0)\n",
    "validation_features = validation_features.fillna(0)\n",
    "functions.get_nan(labeled_aug_features), functions.get_nan(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = labeled_aug_features.drop(['msisdn'], axis=1)\n",
    "y = labeled_aug_features['is_sa']\n",
    "X_validation = validation_features.drop(['msisdn'], axis=1)\n",
    "\n",
    "n_sample = y.shape[0]\n",
    "n_pos_sample = y[y ==1].shape[0]\n",
    "n_neg_sample = y[y == 0].shape[0]\n",
    "print('样本个数：{}; 正样本占{:.2%}; 负样本占{:.2%}'.format(n_sample,\n",
    "                                                   n_pos_sample / n_sample,\n",
    "                                                   n_neg_sample / n_sample))\n",
    "print('特征维数：', X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_aug_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO use all_X to impute\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer2 = SimpleImputer(strategy='most_frequent')\n",
    "X_validation = imputer2.fit_transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_labels) + len(test_labels) == len(labeled_aug_features)\n",
    "len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# 将 msisdn 和 is_sa 并入 X 再划分\n",
    "train_data_msisdn = train_labels['msisdn']\n",
    "test_data_msisdn = test_labels['msisdn']\n",
    "X_df = pd.DataFrame(X, columns=labeled_aug_features.drop(['msisdn'], axis=1).columns)\n",
    "X_df = pd.concat([labeled_aug_features[['msisdn']], X_df], axis=1)\n",
    "train_set = X_df[X_df['msisdn'].isin(train_data_msisdn)][X_df.columns[1:]]\n",
    "test_set = X_df[X_df['msisdn'].isin(test_data_msisdn)][X_df.columns[1:]]\n",
    "\n",
    "print(f\"1 samples / 0 samples in train set: {len(train_set[train_set['is_sa'] == 1])} / {len(train_set[train_set['is_sa'] == 0])}\")\n",
    "print(f\"1 samples / 0 samples in test set: {len(test_set[test_set['is_sa'] == 1])} / {len(test_set[test_set['is_sa'] == 0])}\")\n",
    "\n",
    "if ALL:\n",
    "    train_len = len(test_set) + len(train_set)\n",
    "    test_len = 0\n",
    "else:\n",
    "    train_len, test_len = len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = labeled_aug_features.columns.tolist()\n",
    "columns.remove('msisdn')\n",
    "valid_set = np.c_[X_validation, np.zeros(X_validation.shape[0])]\n",
    "valid_set = pd.DataFrame(valid_set, columns=columns)\n",
    "valid_set['is_sa'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_set = pd.concat([train_set, test_set, valid_set], axis=0).reset_index(drop=True)\n",
    "labeled_data_len = train_set.shape[0] + test_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.shape, train_set.shape, valid_set.shape, all_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_set, valid_set = all_set.iloc[:labeled_data_len].copy(), all_set.iloc[labeled_data_len:].copy()\n",
    "labeled_set.reset_index(drop=True, inplace=True)\n",
    "valid_set.reset_index(drop=True, inplace=True)\n",
    "# 有一些值在SMOTE后对数变换后为 NaN，需要删除这些数据\n",
    "print(labeled_set.isnull().sum().sum())\n",
    "labeled_set = labeled_set.dropna()\n",
    "print(labeled_set.isnull().sum().sum())\n",
    "assert valid_set.shape[0] == validation_features.shape[0]\n",
    "\n",
    "# 重新划分训练集和测试集\n",
    "if not ALL:\n",
    "    train_set, test_set = labeled_set.iloc[:train_len].copy(), labeled_set.iloc[train_len:].copy()\n",
    "    train_set.reset_index(drop=True, inplace=True)\n",
    "    test_set.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    remove_columns = ['cfee+std', 'start_time_diff+start_time_diff_std', 'lfee+mean', 'lfee+sum', 'lfee+std', 'start_time_diff+max']\n",
    "    # remove_columns = ['cfee+std', 'lfee+mean', 'lfee+sum', 'lfee+std']\n",
    "    train_set = train_set.drop(remove_columns, axis=1)\n",
    "    test_set = test_set.drop(remove_columns, axis=1)\n",
    "    valid_set = valid_set.drop(remove_columns, axis=1)\n",
    "    assert train_set.shape[1] == test_set.shape[1] == valid_set.shape[1]\n",
    "   \n",
    "else:\n",
    "    remove_columns = ['cfee+std', 'start_time_diff+start_time_diff_std', 'lfee+mean', 'lfee+sum', 'lfee+std']\n",
    "    labeled_set = labeled_set.drop(remove_columns, axis=1)\n",
    "    valid_set = valid_set.drop(remove_columns, axis=1)\n",
    "    test_set = test_set.drop(remove_columns, axis=1)\n",
    "    assert labeled_set.shape[1] == valid_set.shape[1] == test_set.shape[1]\n",
    "    \n",
    "# 对采样数据做 smote\n",
    "if not NO_SMOTE:\n",
    "    smote = SMOTE(random_state=42)    # 处理过采样的方法\n",
    "    X_train, y_train = smote.fit_resample(train_set.drop(['is_sa'], axis=1), train_set['is_sa'])\n",
    "    train_set = pd.concat([X_train, y_train], axis=1)\n",
    "    print('通过SMOTE方法平衡正负样本后')\n",
    "    n_sample = y_train.shape[0]\n",
    "    n_pos_sample = y_train[y_train == 1].shape[0]\n",
    "    n_neg_sample = y_train[y_train == 0].shape[0]\n",
    "    print('样本个数：{}; 正样本占{:.2%}; 负样本占{:.2%}'.format(n_sample,\n",
    "                                                    n_pos_sample / n_sample,\n",
    "                                                    n_neg_sample / n_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape, test_set.shape, valid_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "if not ALL:\n",
    "    model = TabularPredictor(label='is_sa', eval_metric='f1', problem_type='binary').fit(train_set, presets='medium_quality', time_limit=3600)\n",
    "else:\n",
    "    model = TabularPredictor(label='is_sa', eval_metric='f1', problem_type='binary').fit(labeled_set, presets='best_quality', num_bag_folds=10, time_limit=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ALL:\n",
    "    print(model.evaluate(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model.feature_importance(test_set if not ALL else labeled_set)\n",
    "print(feature_importance)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaderboard\n",
    "if not ALL:\n",
    "    leaderboard = model.leaderboard(test_set, silent=True)\n",
    "    print(leaderboard)\n",
    "else:\n",
    "    leaderboard = model.leaderboard(labeled_set, silent=True)\n",
    "    print(leaderboard)\n",
    "leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "if not ALL:\n",
    "    y_pred = model.predict(test_set)\n",
    "    y_true = test_set['is_sa']\n",
    "    \n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(\"../vis\", exist_ok=True)\n",
    "    plt.savefig(\"../vis/confusion_matrix.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型决策阈值微调\n",
    "threadhold = 0.2\n",
    "if not ALL:\n",
    "    y_pred_proba = model.predict_proba(test_set)\n",
    "    # print(y_pred_proba)\n",
    "    y_pred = (y_pred_proba.iloc[:, 1] > threadhold).astype(int)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 预测\n",
    "y_validation_pred = model.predict(valid_set.drop('is_sa', axis=1))\n",
    "\n",
    "# 将预测结果与 msisdn 对应起来\n",
    "validation_results = validation_features[['msisdn']].copy()\n",
    "validation_results['is_sa'] = y_validation_pred.astype(int)\n",
    "\n",
    "print(validation_results.describe())\n",
    "\n",
    "# 保存结果到CSV文件\n",
    "time_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "file_name = './valid_large_data_{}.csv'.format(time_str) if ALL else './valid_small_data_{}.csv'.format(time_str)\n",
    "validation_results.to_csv(file_name, index=False)\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# f1 最佳阈值调整，找到 best_threshold\n",
    "if not ALL:\n",
    "    y_true = test_set['is_sa']\n",
    "    y_pred_proba = model.predict_proba(test_set).iloc[:, 1]  # 获取正类的预测概率\n",
    "\n",
    "    # 计算不同阈值下的精确率和召回率\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "\n",
    "    # 计算平均精确率\n",
    "    average_precision = average_precision_score(y_true, y_pred_proba)\n",
    "\n",
    "\n",
    "\n",
    "    # 绘制 PR 曲线\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f'PR curve (area = {average_precision:.2f})')\n",
    "    # 画出最佳 f1 分数对应的阈值\n",
    "    best_index = np.argmax(2 * precision * recall / (precision + recall))\n",
    "    best_threshold = thresholds[best_index]\n",
    "    f1 = 2 * precision[best_index] * recall[best_index] / (precision[best_index] + recall[best_index])\n",
    "    plt.plot(recall[best_index], precision[best_index], 'ro', label=f'Best F1 {f1} Threshold: {best_threshold:.2f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best threshold: \", best_threshold)\n",
    "# 使用最佳决策阈值进行预测\n",
    "y_validation_pred_proba = model.predict_proba(valid_set.drop('is_sa', axis=1))\n",
    "y_validation_pred = (y_validation_pred_proba.iloc[:, 1] >= best_threshold).astype(int)\n",
    "\n",
    "# 将预测结果与 msisdn 对应起来\n",
    "validation_results = validation_features[['msisdn']].copy()\n",
    "validation_results['is_sa'] = y_validation_pred.astype(int)\n",
    "\n",
    "print(validation_results.describe())\n",
    "\n",
    "# 保存结果到CSV文件\n",
    "import time\n",
    "time_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "file_name = './valid_large_data_{}.csv'.format(time_str) if ALL else './valid_small_data_{}.csv'.format(time_str)\n",
    "validation_results.to_csv(file_name, index=False)\n",
    "print(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
