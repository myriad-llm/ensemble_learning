{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "\n",
    "ALL = False\n",
    "NO_SMOTE = True\n",
    "TEST_RATIO = 0.3\n",
    "\n",
    "dtypes = {\n",
    "    'msisdn': 'str',\n",
    "    'start_time': 'str',\n",
    "    'end_time': 'str',\n",
    "    'call_event': 'category',\n",
    "    'other_party': 'str',\n",
    "    'ismultimedia': 'category',\n",
    "    'home_area_code': 'str',\n",
    "    'visit_area_code': 'str',\n",
    "    'called_home_code': 'str',\n",
    "    'called_code': 'str',\n",
    "    'a_serv_type': 'int',\n",
    "    'long_type1': 'int',\n",
    "    'roam_type': 'int',\n",
    "    'a_product_id': 'str',\n",
    "    'open_datetime': 'str',\n",
    "    'call_duration': 'int32',\n",
    "    'cfee': 'float64',\n",
    "    'lfee': 'float64',\n",
    "    'hour': 'int8',\n",
    "    'dayofweek': 'int',\n",
    "    'phone1_type': 'int',\n",
    "    'phone2_type': 'int',\n",
    "    'phone1_loc_city': 'str',\n",
    "    'phone1_loc_province': 'str',\n",
    "    'phone2_loc_city': 'str',\n",
    "    'phone2_loc_province': 'str',\n",
    "    'update_time': 'str',\n",
    "    'date': 'str',\n",
    "    'date_c': 'str'\n",
    "}\n",
    "\n",
    "# 判断 processed 文件夹是否存在\n",
    "import os\n",
    "if not os.path.exists('../self_data/processed'):\n",
    "    print(\"Creating processed data folder...\")\n",
    "    # 读取CSV文件\n",
    "    labeled_data = pd.read_csv('../self_data/sorted_trainSet_res_with_head.csv', dtype=dtypes)\n",
    "    labels = pd.read_csv('../self_data/trainSet_ans_with_head.csv', dtype=dtypes)\n",
    "\n",
    "    validation_data = pd.read_csv('../self_data/sorted_validationSet_res_with_head.csv', dtype=dtypes)\n",
    "\n",
    "    # 按照 msisdn 切分 train_data 和 test_data\n",
    "    train_data_msisdn, test_data_msisdn = train_test_split(labels['msisdn'], test_size=TEST_RATIO, random_state=42, stratify=labels['is_sa'])\n",
    "    train_data = labeled_data[labeled_data['msisdn'].isin(train_data_msisdn)]\n",
    "    train_labels = labels[labels['msisdn'].isin(train_data_msisdn)]\n",
    "    assert len(train_data['msisdn'].unique()) == len(train_data_msisdn)\n",
    "\n",
    "    test_data = labeled_data[labeled_data['msisdn'].isin(test_data_msisdn)]\n",
    "    test_labels = labels[labels['msisdn'].isin(test_data_msisdn)]\n",
    "    assert len(test_data['msisdn'].unique()) == len(test_data_msisdn)\n",
    "\n",
    "\n",
    "    # # 遍历 groupby('msisdn') 的结果，对每个 msisdn 进行数据增强\n",
    "    # # ------\n",
    "    from tqdm import tqdm\n",
    "    import os\n",
    "    import sys\n",
    "    from utils.augmentation import Augmentation\n",
    "\n",
    "    addition_train_data = []\n",
    "    addition_train_labels = []\n",
    "\n",
    "    times = 1\n",
    "    # ratio_range = 0.1\n",
    "    ratio_range = (0.01, 0.2)\n",
    "    pbar = tqdm(train_data.groupby('msisdn'))\n",
    "    for msisdn, group in pbar:\n",
    "        if msisdn == 0:\n",
    "            continue\n",
    "        # print(f\"Augmenting msisdn {msisdn}\")\n",
    "        pbar.set_description(f\"Augmenting msisdn {msisdn}\")\n",
    "        label = train_labels[train_labels['msisdn'] == msisdn].iloc[0]['is_sa']\n",
    "        aug = Augmentation(group, label, 'msisdn', 'is_sa')\n",
    "        # 对正负样本进行平衡 样本比 1:4\n",
    "        if label == 1:\n",
    "            res_df, res_labels = aug.times(ratio=ratio_range, times=9*times, method='mask')\n",
    "\n",
    "            addition_train_data.append(res_df)\n",
    "            addition_train_labels.append(res_labels)\n",
    "\n",
    "            # res_df, res_labels = aug.times(window_size=100, step_size=80, times=1, method='sliding_window')\n",
    "\n",
    "            # addition_train_data.append(res_df)\n",
    "            # addition_train_labels.append(res_labels)\n",
    "        # else:\n",
    "        #     res_df, res_labels = aug.times(ratio=ratio_range, times=times, method='mask')\n",
    "\n",
    "        #     addition_train_data.append(res_df)\n",
    "        #     addition_train_labels.append(res_labels)\n",
    "\n",
    "            # res_df, res_labels = aug.times(window_size=100, step_size=80, times=1, method='sliding_window')\n",
    "\n",
    "            # addition_train_data.append(res_df)\n",
    "            # addition_train_labels.append(res_labels)\n",
    "            \n",
    "    addition_train_data = pd.concat(addition_train_data)\n",
    "    addition_train_labels = pd.concat(addition_train_labels)\n",
    "\n",
    "    # 将新数据加入到train_data中\n",
    "    train_data = pd.concat([train_data, addition_train_data], ignore_index=True).reset_index(drop=True)\n",
    "    train_labels = pd.concat([train_labels, addition_train_labels], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    # 按照 msisdn, start_time 排序\n",
    "    sort_start_time = time.time()\n",
    "    train_data = train_data.sort_values(by=['msisdn', 'start_time']).reset_index(drop=True)\n",
    "    train_labels = train_labels.sort_values(by=['msisdn']).reset_index(drop=True)\n",
    "    print('sort time:', time.time() - sort_start_time)\n",
    "\n",
    "    labels_aug = pd.concat([train_labels, test_labels], ignore_index=True).reindex()\n",
    "    # ------------------\n",
    "\n",
    "    # save\n",
    "    print(\"Saving processed data...\")\n",
    "    os.makedirs('../self_data/processed', exist_ok=True)\n",
    "    train_data.to_csv('../self_data/processed/train_data.csv', index=False)\n",
    "    train_labels.to_csv('../self_data/processed/train_labels.csv', index=False)\n",
    "    test_data.to_csv('../self_data/processed/test_data.csv', index=False)\n",
    "    test_labels.to_csv('../self_data/processed/test_labels.csv', index=False)\n",
    "\n",
    "    labels_aug.to_csv('../self_data/processed/labels_aug.csv', index=False)\n",
    "\n",
    "    validation_data.to_csv('../self_data/processed/validation_data.csv', index=False)\n",
    "    # TODO: test_data\n",
    "\n",
    "else:\n",
    "    print(\"Reading processed data...\")\n",
    "    train_data = pd.read_csv('../self_data/processed/train_data.csv', dtype=dtypes)\n",
    "    train_labels = pd.read_csv('../self_data/processed/train_labels.csv', dtype=dtypes)\n",
    "    test_data = pd.read_csv('../self_data/processed/test_data.csv', dtype=dtypes)\n",
    "    test_labels = pd.read_csv('../self_data/processed/test_labels.csv', dtype=dtypes)\n",
    "\n",
    "    labels_aug = pd.read_csv('../self_data/processed/labels_aug.csv', dtype=dtypes)\n",
    "\n",
    "    validation_data = pd.read_csv('../self_data/processed/validation_data.csv', dtype=dtypes)\n",
    "\n",
    "labeled_data_aug = pd.concat([train_data, test_data], ignore_index=True).reindex()\n",
    "assert len(labeled_data_aug['msisdn'].unique()) == len(labels_aug['msisdn'].unique())\n",
    "\n",
    "# 转换时间格式\n",
    "labeled_data_aug['start_time'] = pd.to_datetime(labeled_data_aug['start_time'], format='%Y%m%d%H%M%S')\n",
    "labeled_data_aug['end_time'] = pd.to_datetime(labeled_data_aug['end_time'], format='%Y%m%d%H%M%S')\n",
    "labeled_data_aug['open_datetime'] = pd.to_datetime(labeled_data_aug['open_datetime'], format='%Y%m%d%H%M%S')\n",
    "labeled_data_aug['update_time'] = pd.to_datetime(labeled_data_aug['update_time'])\n",
    "labeled_data_aug['date'] = pd.to_datetime(labeled_data_aug['date'])\n",
    "\n",
    "validation_data['start_time'] = pd.to_datetime(validation_data['start_time'], format='%Y%m%d%H%M%S')\n",
    "validation_data['end_time'] = pd.to_datetime(validation_data['end_time'], format='%Y%m%d%H%M%S')\n",
    "validation_data['open_datetime'] = pd.to_datetime(validation_data['open_datetime'], format='%Y%m%d%H%M%S',errors='coerce')\n",
    "validation_data['update_time'] = pd.to_datetime(validation_data['update_time'])\n",
    "validation_data['date'] = pd.to_datetime(validation_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为每条记录添加start_time_diff，记录 start_time 与上一条记录的 start_time 之差 (单位：秒)\n",
    "start_time_diff = labeled_data_aug.groupby('msisdn')['start_time'].diff().dt.total_seconds().fillna(0).reset_index(drop=True)\n",
    "# 将该列加入到数据集中\n",
    "labeled_data_aug['start_time_diff'] = start_time_diff.copy()\n",
    "# time_diff_start2end = train_data.groupby('msisdn')['end_time'].diff().dt.total_seconds().fillna(0)\n",
    "start_time_diff = validation_data.groupby('msisdn')['start_time'].diff().dt.total_seconds().fillna(0).reset_index(drop=True)\n",
    "validation_data['start_time_diff'] = start_time_diff.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "数据特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 聚合特征\n",
    "def aggregate_features(data):\n",
    "    return data.groupby('msisdn').agg({\n",
    "    'call_duration': [\n",
    "        ('sum', 'sum'), \n",
    "        ('mean', 'mean'), \n",
    "        ('max', 'max'), \n",
    "        ('std', 'std'),\n",
    "        ('quantile_25', lambda x: x.quantile(0.25)), \n",
    "        ('quantile_50', lambda x: x.quantile(0.50)), \n",
    "        ('quantile_75', lambda x: x.quantile(0.75)),\n",
    "    ],\n",
    "    'cfee': [\n",
    "        ('sum', 'sum'),\n",
    "        ('std', 'std'), \n",
    "        ('mean', 'mean'),\n",
    "    ],\n",
    "    'lfee': [\n",
    "        ('sum', 'sum'), \n",
    "        ('mean', 'mean'),\n",
    "        ('std', 'std'),\n",
    "    ],\n",
    "    'hour': [\n",
    "        ('mean', 'mean'), \n",
    "        ('std', 'std'), \n",
    "        ('max', 'max'), \n",
    "        ('min', 'min'),\n",
    "    ],\n",
    "    'dayofweek': [\n",
    "        ('std', 'std'), \n",
    "        ('magic', lambda x: x.value_counts().mean()), \n",
    "        ('work_day_num', lambda x: x[x.isin([1,2,3,4,5])].count()), \n",
    "        ('weekend_num', lambda x: x[x.isin([6,7])].count()),\n",
    "        ('mode', lambda x: x.mode().values[0]),\n",
    "        ('work_day_weekend_diff', lambda x: (x[x.isin([1,2,3,4,5])].count() - x[x.isin([6,7])].count()) / (x[x.isin([1,2,3,4,5])].count() + x[x.isin([6,7])].count())),\n",
    "    ],\n",
    "    # 'home_area_code': [\n",
    "    #     ('home_area_code_nunique', 'nunique')\n",
    "    # ],\n",
    "    'visit_area_code': [\n",
    "        ('nunique', 'nunique'),\n",
    "        ('times_not_at_home_area', lambda x: x[x != x.shift()].count()/x.count())\n",
    "    ],\n",
    "    'called_home_code': [\n",
    "        ('nunique', 'nunique'),\n",
    "        ('called_diff_home_code', lambda x: x[x != x.shift()].count() / x.count())\n",
    "    ],\n",
    "    'called_code': [\n",
    "        ('nunique', 'nunique'),\n",
    "        ('diff', lambda x: x[x != x.shift()].count()/ x.count())\n",
    "    ],\n",
    "    'open_datetime': [\n",
    "        ('open_count', 'nunique')\n",
    "    ],\n",
    "    'other_party': [\n",
    "        ('account_person_num', 'nunique'),\n",
    "        ('called_diff_home_code', lambda x: x[x != x.shift()].count() / x.count())\n",
    "    ],\n",
    "    'a_serv_type': [\n",
    "        ('call_num', lambda x: x[x.isin([1, 3])].count()), \n",
    "        ('called_num', lambda x: x[x == 2].count()),\n",
    "        ('call_called_normalized_diff', lambda x: (x[x.isin([1, 3])].count() - x[x == 2].count()) /  (x[x.isin([1, 3])].count() + x[x == 2].count())),\n",
    "    ],\n",
    "    'start_time_diff': [\n",
    "        ('start_time_diff_mean', 'mean'), \n",
    "        ('start_time_diff_std', 'std'), \n",
    "        ('max', 'max'), \n",
    "        ('coefficient_of_variation', lambda x: x.std() / x.mean()),\n",
    "    ], \n",
    "    # 'phone1_type': [\n",
    "    #     ('nunique', 'nunique'),\n",
    "    #     ('mode', lambda x: x.mode().values[0])\n",
    "    # ],\n",
    "    # 'distance': [\n",
    "    #     ('sum', 'sum'), \n",
    "    #     ('std', 'std'), \n",
    "    #     ('max', 'max'), \n",
    "    #     ('quantile_25', lambda x: x.quantile(0.25)), \n",
    "    #     ('quantile_50', lambda x: x.quantile(0.50)), \n",
    "    #     ('quantile_75', lambda x: x.quantile(0.75)),\n",
    "    # ]\n",
    "})\n",
    "\n",
    "labeled_aug_features = aggregate_features(labeled_data_aug)\n",
    "validation_features = aggregate_features(validation_data)\n",
    "\n",
    "labeled_aug_features.columns = ['+'.join(col).strip() for col in labeled_aug_features.columns.values]\n",
    "validation_features.columns = ['+'.join(col).strip() for col in validation_features.columns.values]\n",
    "\n",
    "labeled_aug_features.columns = labeled_aug_features.columns.str.replace('[', '').str.replace(']', '').str.replace('<', '').str.replace('>', '').str.replace('(', '').str.replace(')', '').str.replace(',', '').str.replace(' ', '_')\n",
    "validation_features.columns = validation_features.columns.str.replace('[', '').str.replace(']', '').str.replace('<', '').str.replace('>', '').str.replace('(', '').str.replace(')', '').str.replace(',', '').str.replace(' ', '_')\n",
    "\n",
    "# 重置索引\n",
    "labeled_aug_features = labeled_aug_features.reset_index()\n",
    "validation_features = validation_features.reset_index()\n",
    "\n",
    "# 合并标签数据\n",
    "labeled_aug_features = labeled_aug_features.merge(labels_aug, on='msisdn', how='left')\n",
    "# 打印结果\n",
    "# labeled_aug_features\n",
    "\n",
    "# # 添加 ae 的编码特征\n",
    "# labeled_ae = pd.read_csv('../data/ae/train.csv', dtype=dtypes)\n",
    "# valid_ae = pd.read_csv('../data/ae/val.csv', dtype=dtypes)\n",
    "# labeled_aug_features = labeled_aug_features.merge(labeled_ae, on='msisdn', how='left')\n",
    "# validation_features = validation_features.merge(valid_ae, on='msisdn', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_aug_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labeled_aug_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nan(train):\n",
    "    # 获取 train 中的 nan值\n",
    "    train_nan = train[train.isnull().T.any()]\n",
    "    # 统计 每列含有的 nan 数量\n",
    "    for col in train.columns:\n",
    "        if train[col].isnull().sum() > 0:\n",
    "            print(col, train[col].isnull().sum())\n",
    "\n",
    "    return train_nan\n",
    "get_nan(labeled_aug_features), get_nan(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一般只有 std 会出现 nan 值故所有的 nan 值填充为 0\n",
    "labeled_aug_features = labeled_aug_features.fillna(0)\n",
    "validation_features = validation_features.fillna(0)\n",
    "\n",
    "def get_nan(train):\n",
    "    # 获取 train 中的 nan值\n",
    "    train_nan = train[train.isnull().T.any()]\n",
    "    # 统计 每列含有的 nan 数量\n",
    "    for col in train.columns:\n",
    "        if train[col].isnull().sum() > 0:\n",
    "            print(col, train[col].isnull().sum())\n",
    "\n",
    "    return train_nan\n",
    "get_nan(labeled_aug_features), get_nan(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 交叉特征\n",
    "# # # 将所有特征两两相乘\n",
    "# from itertools import combinations\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def cross_features(data):\n",
    "#     cross_features = []\n",
    "#     new_features = []\n",
    "#     cross_cols = data.columns.tolist()\n",
    "#     rm_cols = ['msisdn', 'is_sa']\n",
    "#     for col in rm_cols:\n",
    "#         if col in cross_cols:\n",
    "#             cross_cols.remove(col)\n",
    "\n",
    "#     for i, j in tqdm(combinations(cross_cols, 2), total=len(cross_cols) * (len(cross_cols) - 1) // 2):\n",
    "#         new_features.append(data[i] * data[j])\n",
    "#         cross_features.append(f'{i}_cross_{j}')\n",
    "#     new_features = pd.concat(new_features, axis=1)\n",
    "#     new_features.columns = cross_features\n",
    "#     data = pd.concat([data, new_features], axis=1)\n",
    "#     return data, cross_features\n",
    "\n",
    "# labeled_aug_features, _ = cross_features(labeled_aug_features)\n",
    "# validation_features, _ = cross_features(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = labeled_aug_features.drop(['msisdn'], axis=1)\n",
    "y = labeled_aug_features['is_sa']\n",
    "X_validation = validation_features.drop(['msisdn'], axis=1)\n",
    "\n",
    "n_sample = y.shape[0]\n",
    "n_pos_sample = y[y ==1].shape[0]\n",
    "n_neg_sample = y[y == 0].shape[0]\n",
    "print('样本个数：{}; 正样本占{:.2%}; 负样本占{:.2%}'.format(n_sample,\n",
    "                                                   n_pos_sample / n_sample,\n",
    "                                                   n_neg_sample / n_sample))\n",
    "print('特征维数：', X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_aug_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO use all_X to impute\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer2 = SimpleImputer(strategy='most_frequent')\n",
    "X_validation = imputer2.fit_transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_labels) + len(test_labels) == len(labeled_aug_features)\n",
    "len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# 将 msisdn 和 is_sa 并入 X 再划分\n",
    "train_data_msisdn = train_labels['msisdn']\n",
    "test_data_msisdn = test_labels['msisdn']\n",
    "X_df = pd.DataFrame(X, columns=labeled_aug_features.drop(['msisdn'], axis=1).columns)\n",
    "X_df = pd.concat([labeled_aug_features[['msisdn']], X_df], axis=1)\n",
    "train_set = X_df[X_df['msisdn'].isin(train_data_msisdn)][X_df.columns[1:]]\n",
    "test_set = X_df[X_df['msisdn'].isin(test_data_msisdn)][X_df.columns[1:]]\n",
    "\n",
    "print(f\"1 samples / 0 samples in train set: {len(train_set[train_set['is_sa'] == 1])} / {len(train_set[train_set['is_sa'] == 0])}\")\n",
    "print(f\"1 samples / 0 samples in test set: {len(test_set[test_set['is_sa'] == 1])} / {len(test_set[test_set['is_sa'] == 0])}\")\n",
    "\n",
    "if ALL:\n",
    "    # if not NO_SMOTE:\n",
    "    #     smote = SMOTE(random_state=42)    # 处理过采样的方法\n",
    "    #     X, y = smote.fit_resample(X, y)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42, shuffle=True)\n",
    "    train_len = len(test_set) + len(train_set)\n",
    "    test_len = 0\n",
    "else:\n",
    "    # X_train,X_test,y_train,y_test = train_test_split(X,y,stratify = y,test_size= 0.3,random_state=42, shuffle=True)\n",
    "    # X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "    train_len, test_len = len(train_set), len(test_set)\n",
    "\n",
    "    # if not NO_SMOTE:\n",
    "    #     smote = SMOTE(random_state=42)    # 处理过采样的方法\n",
    "    #     X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    #     print('通过SMOTE方法平衡正负样本后')\n",
    "    #     n_sample = y_train.shape[0]\n",
    "    #     n_pos_sample = y_train[y_train == 1].shape[0]\n",
    "    #     n_neg_sample = y_train[y_train == 0].shape[0]\n",
    "    #     print('样本个数：{}; 正样本占{:.2%}; 负样本占{:.2%}'.format(n_sample,\n",
    "    #                                                     n_pos_sample / n_sample,\n",
    "    #                                                     n_neg_sample / n_sample))\n",
    "    #     print('特征维数：', X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = labeled_aug_features.columns.tolist()\n",
    "columns.remove('msisdn')\n",
    "valid_set = np.c_[X_validation, np.zeros(X_validation.shape[0])]\n",
    "valid_set = pd.DataFrame(valid_set, columns=columns)\n",
    "valid_set['is_sa'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_set = pd.concat([train_set, test_set, valid_set], axis=0).reset_index(drop=True)\n",
    "labeled_data_len = train_set.shape[0] + test_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.shape, train_set.shape, valid_set.shape, all_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_SMOTE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_set, valid_set = all_set.iloc[:labeled_data_len].copy(), all_set.iloc[labeled_data_len:].copy()\n",
    "labeled_set.reset_index(drop=True, inplace=True)\n",
    "valid_set.reset_index(drop=True, inplace=True)\n",
    "# 有一些值在SMOTE后对数变换后为 NaN，需要删除这些数据\n",
    "print(labeled_set.isnull().sum().sum())\n",
    "labeled_set = labeled_set.dropna()\n",
    "print(labeled_set.isnull().sum().sum())\n",
    "assert valid_set.shape[0] == validation_features.shape[0]\n",
    "\n",
    "# 重新划分训练集和测试集\n",
    "if not ALL:\n",
    "    train_set, test_set = labeled_set.iloc[:train_len].copy(), labeled_set.iloc[train_len:].copy()\n",
    "    train_set.reset_index(drop=True, inplace=True)\n",
    "    test_set.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # remove_columns = ['distance_distance_std', \"start_time_diff_start_time_diff_max\", \"distance_distance_quantile_75\"]\n",
    "\n",
    "    # remove_columns = [\"lfee_lfee_std\", \"lfee_lfee_mean\", 'call_duration_call_duration_max', \"distance_distance_quantile_50\", \"call_duration_call_duration_quantile_25\"]\n",
    "    # remove_columns = [\"7\", \"6\", \"lfee_lfee_mean\", \"hour_hour_std\", \"1\", \"call_duration_call_duration_quantile_75\", \"3\", \"cfee_cfee_std\", \"start_time_diff_start_time_diff_max\", \"call_duration_call_duration_max\", \"dayofweek_dayofweek_mode\", \"distance_distance_quantile_75\", \"cfee_cfee_mean\"] # , \"visit_area_code_visit_area_code_nunique\", \"visit_area_code_visit_area_code_nunique\"\n",
    "    # remove_columns = ['visit_area_code+nunique_cross_start_time_diff+max', \"distance+std\"]\n",
    "    # remove_columns = ['dayofweek+std', 'start_time_diff+max', 'distance+quantile_75', 'lfee+mean', 'lfee+std', 'lfee+sum', 'cfee+sum', '6', 'visit_area_code+nunique']\n",
    "    remove_columns = ['cfee+std', 'start_time_diff+start_time_diff_std', 'lfee+mean', 'lfee+sum', 'lfee+std', 'start_time_diff+max']\n",
    "    train_set = train_set.drop(remove_columns, axis=1)\n",
    "    test_set = test_set.drop(remove_columns, axis=1)\n",
    "    valid_set = valid_set.drop(remove_columns, axis=1)\n",
    "    assert train_set.shape[1] == test_set.shape[1] == valid_set.shape[1]\n",
    "   \n",
    "else:\n",
    "    remove_columns = ['cfee+std', 'start_time_diff+start_time_diff_std', 'lfee+mean', 'lfee+sum', 'lfee+std']\n",
    "    labeled_set = labeled_set.drop(remove_columns, axis=1)\n",
    "    valid_set = valid_set.drop(remove_columns, axis=1)\n",
    "    test_set = test_set.drop(remove_columns, axis=1)\n",
    "    assert labeled_set.shape[1] == valid_set.shape[1] == test_set.shape[1]\n",
    "    \n",
    "# 对采样数据做 smote\n",
    "if not NO_SMOTE:\n",
    "    smote = SMOTE(random_state=42)    # 处理过采样的方法\n",
    "    X_train, y_train = smote.fit_resample(train_set.drop(['is_sa'], axis=1), train_set['is_sa'])\n",
    "    train_set = pd.concat([X_train, y_train], axis=1)\n",
    "    print('通过SMOTE方法平衡正负样本后')\n",
    "    n_sample = y_train.shape[0]\n",
    "    n_pos_sample = y_train[y_train == 1].shape[0]\n",
    "    n_neg_sample = y_train[y_train == 0].shape[0]\n",
    "    print('样本个数：{}; 正样本占{:.2%}; 负样本占{:.2%}'.format(n_sample,\n",
    "                                                    n_pos_sample / n_sample,\n",
    "                                                    n_neg_sample / n_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape, test_set.shape, valid_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 autogluon 训练\n",
    "from autogluon.tabular import TabularPredictor\n",
    "# import ray\n",
    "# 使用防止过拟合的超参数\n",
    "# hyperparameters = {\n",
    "#     'GBM': {'lambda_l1': 1e-2, 'lambda_l2': 1e-2},\n",
    "#     'FASTAI': {'dropout_prob': 0.2}\n",
    "# }\n",
    "# ray.shutdown()\n",
    "# ray.init(include_dashboard=True, object_store_memory=10**9)  # Increase object store memory\n",
    "\n",
    "# 输入数据X_train, y_train\n",
    "if not ALL:\n",
    "    # 交叉验证训练\n",
    "    model = TabularPredictor(label='is_sa', eval_metric='f1', problem_type='binary').fit(train_set, presets='medium_quality', time_limit=3600)\n",
    "    # , excluded_model_types=['KNN']\n",
    "    # model = TabularPredictor(label='is_sa', eval_metric='f1', problem_type='binary').fit(train_set, presets='best_quality', time_limit=3600)\n",
    "else:\n",
    "    model = TabularPredictor(label='is_sa', eval_metric='f1', problem_type='binary').fit(labeled_set, presets='best_quality', num_bag_folds=10, time_limit=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ALL:\n",
    "    print(model.evaluate(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model.feature_importance(test_set if not ALL else labeled_set)\n",
    "print(feature_importance)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaderboard\n",
    "if not ALL:\n",
    "    leaderboard = model.leaderboard(test_set, silent=True)\n",
    "    print(leaderboard)\n",
    "else:\n",
    "    leaderboard = model.leaderboard(labeled_set, silent=True)\n",
    "    print(leaderboard)\n",
    "leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在testset 上计算指标\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "if not ALL:\n",
    "    y_pred = model.predict(test_set)\n",
    "    y_true = test_set['is_sa']\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型决策阈值微调\n",
    "threadhold = 0.2\n",
    "if not ALL:\n",
    "    y_pred_proba = model.predict_proba(test_set)\n",
    "    # print(y_pred_proba)\n",
    "    y_pred = (y_pred_proba.iloc[:, 1] > threadhold).astype(int)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 预测\n",
    "y_validation_pred = model.predict(valid_set.drop('is_sa', axis=1))\n",
    "\n",
    "# 将预测结果与 msisdn 对应起来\n",
    "validation_results = validation_features[['msisdn']].copy()\n",
    "validation_results['is_sa'] = y_validation_pred.astype(int)\n",
    "\n",
    "print(validation_results.describe())\n",
    "\n",
    "# 保存结果到CSV文件\n",
    "import time\n",
    "time_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "file_name = './valid_large_data_{}.csv'.format(time_str) if ALL else './valid_small_data_{}.csv'.format(time_str)\n",
    "validation_results.to_csv(file_name, index=False)\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# # 自动调整最佳阈值\n",
    "# if not ALL:\n",
    "#     y_true = test_set['is_sa']\n",
    "#     y_pred_proba = model.predict_proba(test_set)\n",
    "\n",
    "#     thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "#     f1_scores = []\n",
    "\n",
    "#     for threshold in thresholds:\n",
    "#         y_pred = (y_pred_proba.iloc[:, 1] >= threshold).astype(int)\n",
    "#         f1 = f1_score(y_true, y_pred)\n",
    "#         f1_scores.append(f1)\n",
    "\n",
    "#     best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "#     print(f'最佳阈值：{best_threshold}')\n",
    "#     print(f'最佳 F1 分数：{max(f1_scores)}')\n",
    "\n",
    "#     # 使用最佳阈值进行预测\n",
    "#     y_pred = (y_pred_proba.iloc[:, 1] >= best_threshold).astype(int)\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "#     print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 阈值微调版结果\n",
    "# best_threshold = 0.3\n",
    "# # 使用最佳决策阈值进行预测\n",
    "# y_validation_pred_proba = model.predict_proba(valid_set.drop('is_sa', axis=1))\n",
    "# y_validation_pred = (y_validation_pred_proba.iloc[:, 1] >= best_threshold).astype(int)\n",
    "\n",
    "# # 将预测结果与 msisdn 对应起来\n",
    "# validation_results = validation_features[['msisdn']].copy()\n",
    "# validation_results['is_sa'] = y_validation_pred.astype(int)\n",
    "\n",
    "# print(validation_results.describe())\n",
    "\n",
    "# # 保存结果到CSV文件\n",
    "# import time\n",
    "# time_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "# file_name = './valid_large_data_{}.csv'.format(time_str) if ALL else './valid_small_data_{}.csv'.format(time_str)\n",
    "# validation_results.to_csv(file_name, index=False)\n",
    "# print(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
